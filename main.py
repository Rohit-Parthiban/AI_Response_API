import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List 
from openai import AsyncOpenAI # Import the AsyncOpenAI client
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Get the API key and set up the OpenAI client
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise EnvironmentError("OPENAI_API_KEY not found in .env file")

client = AsyncOpenAI(api_key=api_key)


# --- Pydantic Models for Data Validation ---

class InterviewInput(BaseModel):
    """
    Defines the expected JSON body for the /generate-followups endpoint.
    """
    question: str
    answer: str
    role: Optional[str] = None
    interview_type: Optional[List[str]] = None


class FollowUpData(BaseModel):
    """
    Defines the nested 'data' object in the response.
    """
    followup_question: str

class FollowUpResponse(BaseModel):
    """
    Defines the full JSON response body.
    """
    result: str
    message: str
    data: FollowUpData

# --- End of Models ---


# Create an instance of the FastAPI application
app = FastAPI(
    title="Woyage AI Interview Assistant",
    description="Generates follow-up interview questions.",
    version="1.0.0"
)


@app.get("/")
def read_root():
    """
    Root endpoint to check if the API is running.
    """
    return {"status": "ok", "message": "Woyage AI API is running!"}


# --- THIS IS THE UPDATED ENDPOINT ---

@app.post("/interview/generate-followups", response_model=FollowUpResponse)
async def generate_followups(interview_data: InterviewInput): # Added 'async'
    """
    Accepts interview question and answer, then returns a real follow-up question
    generated by the OpenAI API.
    """

    # --- Create the prompt for the AI ---
    # This is a crucial part. We give the AI a role and a clear task.
    system_prompt = """
    You are an expert interviewer for a tech company. 
    Your goal is to ask insightful, concise, and relevant follow-up questions based on a candidate's answer.
    The follow-up question should be brief and directly usable by the interviewer.
    Do NOT return more than one question. Just return the single best follow-up question.
    """

    # Build the user prompt from the input data
    user_prompt = f"""
    Original Question: "{interview_data.question}"
    Candidate's Answer: "{interview_data.answer}"
    """

    # Add context if it's provided
    if interview_data.role:
        user_prompt += f"\nCandidate's Target Role: {interview_data.role}"
    if interview_data.interview_type:
        user_prompt += f"\nInterview Type: {', '.join(interview_data.interview_type)}"

    # --- Real OpenAI API Call ---
    try:
        completion = await client.chat.completions.create(
            model="gpt-3.5-turbo", # A fast and capable model
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7, # Allows for some creativity
            max_tokens=100 # Keep the response short
        )

        # Extract the follow-up question from the response
        followup_question = completion.choices[0].message.content.strip()

    except Exception as e:
        # Handle potential API errors
        print(f"Error calling OpenAI: {e}")
        raise HTTPException(
            status_code=500, 
            detail="Failed to generate follow-up question from AI service."
        )

    # --- End of API Call ---

    # Format the response to match the FollowUpResponse model
    return {
        "result": "success",
        "message": "Follow-up question generated.",
        "data": {
            "followup_question": followup_question
        }
    }